{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbad70fb-2348-434f-8ac4-4a5ddb5c9159",
   "metadata": {},
   "source": [
    "# Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be6b279-e18d-4bbb-a946-1842b3b5dddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in ./venv/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: protobuf==3.20 in ./venv/lib/python3.10/site-packages (3.20.0)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: lightning in ./venv/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.10/site-packages (8.0.6)\n",
      "Requirement already satisfied: torchmetrics in ./venv/lib/python3.10/site-packages (0.11.4)\n",
      "Requirement already satisfied: optuna in ./venv/lib/python3.10/site-packages (3.1.1)\n",
      "Requirement already satisfied: optuna-dashboard in ./venv/lib/python3.10/site-packages (0.9.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in ./venv/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (2.30.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./venv/lib/python3.10/site-packages (from wandb) (1.21.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in ./venv/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in ./venv/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from wandb) (67.6.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in ./venv/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: Jinja2<5.0 in ./venv/lib/python3.10/site-packages (from lightning) (3.1.2)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in ./venv/lib/python3.10/site-packages (from lightning) (1.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in ./venv/lib/python3.10/site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in ./venv/lib/python3.10/site-packages (from lightning) (1.3.14)\n",
      "Requirement already satisfied: dateutils<2.0 in ./venv/lib/python3.10/site-packages (from lightning) (0.6.12)\n",
      "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in ./venv/lib/python3.10/site-packages (from lightning) (6.3.0)\n",
      "Requirement already satisfied: fastapi<0.89.0,>=0.69.0 in ./venv/lib/python3.10/site-packages (from lightning) (0.88.0)\n",
      "Requirement already satisfied: fsspec<2024.0,>=2022.5.0 in ./venv/lib/python3.10/site-packages (from lightning) (2023.4.0)\n",
      "Requirement already satisfied: inquirer<5.0,>=2.10.0 in ./venv/lib/python3.10/site-packages (from lightning) (3.1.3)\n",
      "Requirement already satisfied: lightning-cloud>=0.5.34 in ./venv/lib/python3.10/site-packages (from lightning) (0.5.34)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from lightning) (0.8.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in ./venv/lib/python3.10/site-packages (from lightning) (1.24.3)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from lightning) (23.1)\n",
      "Requirement already satisfied: pydantic<4.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from lightning) (1.10.7)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in ./venv/lib/python3.10/site-packages (from lightning) (13.3.5)\n",
      "Requirement already satisfied: starlette in ./venv/lib/python3.10/site-packages (from lightning) (0.22.0)\n",
      "Requirement already satisfied: starsessions<2.0,>=1.2.1 in ./venv/lib/python3.10/site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in ./venv/lib/python3.10/site-packages (from lightning) (2.1.0.dev20230504)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in ./venv/lib/python3.10/site-packages (from lightning) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in ./venv/lib/python3.10/site-packages (from lightning) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3.0 in ./venv/lib/python3.10/site-packages (from lightning) (2.0.2)\n",
      "Requirement already satisfied: uvicorn<2.0 in ./venv/lib/python3.10/site-packages (from lightning) (0.22.0)\n",
      "Requirement already satisfied: websocket-client<3.0 in ./venv/lib/python3.10/site-packages (from lightning) (1.5.1)\n",
      "Requirement already satisfied: websockets<12.0 in ./venv/lib/python3.10/site-packages (from lightning) (11.0.2)\n",
      "Requirement already satisfied: pytorch-lightning in ./venv/lib/python3.10/site-packages (from lightning) (2.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./venv/lib/python3.10/site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.10/site-packages (from ipywidgets) (8.13.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in ./venv/lib/python3.10/site-packages (from ipywidgets) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in ./venv/lib/python3.10/site-packages (from ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./venv/lib/python3.10/site-packages (from optuna) (1.10.4)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in ./venv/lib/python3.10/site-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: colorlog in ./venv/lib/python3.10/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./venv/lib/python3.10/site-packages (from optuna) (2.0.12)\n",
      "Requirement already satisfied: bottle in ./venv/lib/python3.10/site-packages (from optuna-dashboard) (0.12.25)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from optuna-dashboard) (1.2.2)\n",
      "Requirement already satisfied: Mako in ./venv/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in ./venv/lib/python3.10/site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.10/site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: pytz in ./venv/lib/python3.10/site-packages (from dateutils<2.0->lightning) (2023.3)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in ./venv/lib/python3.10/site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./venv/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./venv/lib/python3.10/site-packages (from starlette->lightning) (3.6.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./venv/lib/python3.10/site-packages (from fsspec<2024.0,>=2022.5.0->lightning) (3.8.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: blessed>=1.19.0 in ./venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: python-editor>=1.0.4 in ./venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
      "Requirement already satisfied: readchar>=3.0.6 in ./venv/lib/python3.10/site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
      "Requirement already satisfied: appnope in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: pyzmq>=20 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in ./venv/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\n",
      "Requirement already satisfied: backcall in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from Jinja2<5.0->lightning) (2.1.2)\n",
      "Requirement already satisfied: pyjwt in ./venv/lib/python3.10/site-packages (from lightning-cloud>=0.5.34->lightning) (2.6.0)\n",
      "Requirement already satisfied: python-multipart in ./venv/lib/python3.10/site-packages (from lightning-cloud>=0.5.34->lightning) (0.0.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in ./venv/lib/python3.10/site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.12.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch<4.0,>=1.11.0->lightning) (3.1)\n",
      "Requirement already satisfied: h11>=0.8 in ./venv/lib/python3.10/site-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./venv/lib/python3.10/site-packages (from scikit-learn->optuna-dashboard) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./venv/lib/python3.10/site-packages (from scikit-learn->optuna-dashboard) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn->optuna-dashboard) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in ./venv/lib/python3.10/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./venv/lib/python3.10/site-packages (2.1.0.dev20230504)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.10/site-packages (0.16.0.dev20230504)\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.10/site-packages (2.1.0.dev20230504)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch<3,>=2.0) (2023.4.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from torchvision) (2.30.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->torchvision) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch<3,>=2.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Update Pip\n",
    "%pip install --quiet -U pip\n",
    "\n",
    "# Install deps\n",
    "%pip install wandb protobuf==3.20 tqdm lightning ipywidgets torchmetrics optuna optuna-dashboard\n",
    "\n",
    "# Update to pytorch 2.x\n",
    "%pip install -U \"torch>=2.0,<3\" torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aee402-f8b0-476c-8f64-01f760e7db89",
   "metadata": {},
   "source": [
    "# Sanity\n",
    "\n",
    "If this fails, make sure you run on a device with an NVIDIA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1058ed95-0498-4ade-9dee-4f4007178076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6de96-8734-4049-918b-85ec8df8f503",
   "metadata": {},
   "source": [
    "# Login to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8ebc92-3668-45fd-9ecb-4ca485a75d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoof-golan\u001b[0m (\u001b[33mmlab-tlv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a32851",
   "metadata": {},
   "source": [
    "# Housekeeping ğŸ ğŸ§¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d4d7646-15fd-482a-a91b-115f7094c682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Training\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "# Logging\n",
    "import wandb\n",
    "\n",
    "# Hyper-Parameter Search\n",
    "import optuna\n",
    "from optuna_lightning_helper import PyTorchLightningPruningCallback\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8052a-195a-4349-b648-393ff36399ed",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "| Parameter  | Description | \n",
    "| ---------- | ------------|\n",
    "| `BATCH_SIZE`                                    | Make as big as you can fit in VRAM |\n",
    "| `torch.set_float32_matmul_precision('medium')`  | To speed up comutations |\n",
    "| `PATH_DATASETS` | Where to store the datasets on disk (TODO: maybe use `ramfs`) |\n",
    "| `OPTUNA_DATABASE_URL` | Where to store hyper-parameter search progress (this is resumable keep this file handy) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e93a7f7-2ef9-426a-a225-a5e4c8848a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PROJECT_NAME': 'soof-autoencoder-v6',\n",
       " 'BATCH_SIZE': 2048,\n",
       " 'OPTUNA_DATABASE_URL': 'sqlite:///db.sqlite3',\n",
       " 'PATH_DATASETS': 'data'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "PATH_DATASETS = \"data\"\n",
    "OPTUNA_DATABASE_URL = \"sqlite:///db.sqlite3\"\n",
    "PROJECT_NAME = \"soof-autoencoder-v6\"\n",
    "\n",
    "dict(\n",
    "    PROJECT_NAME=PROJECT_NAME,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    OPTUNA_DATABASE_URL=OPTUNA_DATABASE_URL,\n",
    "    PATH_DATASETS=PATH_DATASETS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24efe1-8b0f-45e8-931d-8b0b57d85a86",
   "metadata": {},
   "source": [
    "# Load dataset ğŸ“¦ğŸ“¦ğŸ“¦\n",
    "\n",
    "A [LightningDataModule][dm] that wraps the [FashionMNIST][ds] dataset\n",
    "\n",
    "![Fashion MNIST sprite sheet][sprite]\n",
    "\n",
    "[sprite]: https://github.com/zalandoresearch/fashion-mnist/blob/c29cd591aa1b867e4b59227ee9a08ed0d4d4b34d/doc/img/fashion-mnist-sprite.png?raw=true\n",
    "[dm]: https://lightning.ai/docs/pytorch/stable/data/datamodule.html\n",
    "[ds]: https://github.com/zalandoresearch/fashion-mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045a52c-66b6-4976-82ea-fa041ef5613c",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "During training the data is transformed with:\n",
    "\n",
    "* Normalization - standardizing the mean and std of the samples\n",
    "* TODO: Salt-and-peper noise\n",
    "* TODO: Gaussian noise\n",
    "* TODO: Blurs\n",
    "* TODO: Crops\n",
    "* TODO: Shifts\n",
    "* TODO: Warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cdf2f14f-6e50-4819-aa66-f29b9a03e876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FashionMNISTDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = PATH_DATASETS, batch_size: int = 512, **kw):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # See transforms in the docs above\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),  # Magic numbers from FashionMNIST\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Download the dataset\n",
    "        FashionMNIST(self.data_dir, train=True, download=True)\n",
    "        FashionMNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = FashionMNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.train_data, self.val_data = random_split(mnist_full, [59000, 1000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_data = FashionMNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, shuffle=True, batch_size=self.batch_size, pin_memory=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size, pin_memory=True, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, pin_memory=True, num_workers=0)\n",
    "    \n",
    "    def sample_train(self, num: int):\n",
    "        self.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83a58437-314d-4b7f-8846-3e8e9f9e8b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, size: int = 28, latent_dim: int = 16, act_fn: object = nn.GELU, **kw):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        factors = [2, 4, 8]\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Flatten(start_dim=2),\n",
    "            nn.Linear(in_features=size*size, out_features=latent_dim * factors[2]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[2], out_features=latent_dim * factors[1]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[1], out_features=latent_dim * factors[0]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[0], out_features=latent_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.seq(x)\n",
    "        \n",
    "class MLPDecoder(nn.Sequential):\n",
    "    def __init__(self, size: int = 28, latent_dim: int = 16, act_fn: object = nn.GELU, **kw):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        factors = [2, 4, 8]\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_dim, out_features=latent_dim * factors[0]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[0], out_features=latent_dim * factors[1]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[1], out_features=latent_dim * factors[2]), act_fn(),\n",
    "            nn.Linear(in_features=latent_dim * factors[2], out_features=size*size), nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, *rest = x.shape\n",
    "        return self.seq(x).view(b, c, self.size, self.size)\n",
    "\n",
    "class MLPAutoencoder(nn.Module):\n",
    "    def __init__(self, size: int = 28, latent_dim: int = 16, act_fn: object = nn.GELU, **kw):\n",
    "        super().__init__()\n",
    "        self.encoder = MLPEncoder(size=size, latent_dim=latent_dim, act_fn=act_fn, **kw)\n",
    "        self.decoder = MLPDecoder(size=size, latent_dim=latent_dim, act_fn=act_fn, **kw)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(self.encoder(x))\n",
    "        \n",
    "assert MLPAutoencoder()(torch.rand((2,1,28,28))).shape == (2,1,28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057ab12-4e6b-4c01-8bc6-32e4edab3b89",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This is a convolution based auto-encoder\n",
    "\n",
    "[Excalidraw](https://excalidraw.com/#json=s5Oy3xAQNIlHolsjPFct0,QyBB8WzaIzGBG1Q5IeD1uw)\n",
    "\n",
    "![](docs/auto-encoder.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a35e6de-93b8-4407-9a06-4efb163185f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 28x28 => 14x14\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 14x14 => 7x7\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 7x7 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2bbc630-6d62-461c-a1a2-e4ad8726475a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # 4x4 => 7x7\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid,\n",
    "                2 * c_hid, kernel_size=3, \n",
    "                output_padding=0,          # NOTE! This was modified to support 28x28 images, instead of 32x32\n",
    "                padding=1, \n",
    "                stride=2\n",
    "            ),  \n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            \n",
    "            # 7x7 => 14x14\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            \n",
    "            # 14x14 => 28x28\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6c63052-4c97-47af-821b-057c6bf65a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "        num_input_channels: int = 3,\n",
    "        width: int = 32,\n",
    "        height: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_input_channels = num_input_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a210b1a9-d75c-4b17-8972-f6748f13285c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(*a,**kw):\n",
    "    return MLPAutoencoder()\n",
    "    return Autoencoder(\n",
    "            num_input_channels=1,\n",
    "            width=28,\n",
    "            height=28,\n",
    "            base_channel_size=32,  # TODO: hyperparam?\n",
    "            latent_dim=64,  # TODO: hyperparam?\n",
    "        )\n",
    "\n",
    "assert isinstance(create_model(), nn.Module), f\"Expected nn.Module got {type(create_model())}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09d84e-ebd5-435c-a22a-b72db6c5a2d8",
   "metadata": {},
   "source": [
    "# Lightning Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0698113a-6ee5-48a0-96a6-54de0db29e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LightningAutoencoder(L.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        model = create_model(\"autoencoder\")\n",
    "        self.model = model\n",
    "        self.save_hyperparameters()\n",
    "        self.example_input_array = torch.zeros(1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "        \n",
    "        TODO: We probably can get fancier than MSE, if we think this will help\n",
    "        \"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        b, c, h, w = x.shape\n",
    "        x_hat = self.forward(x)\n",
    "        \n",
    "        x = x.view(b, -1)\n",
    "        x_hat = x_hat.view(b, -1)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"mean\")\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            # self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=self.hparams.momentum,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e872ef-da21-444e-baa7-572d2df68c16",
   "metadata": {},
   "source": [
    "# Training Loop (Managed by Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fb6a7bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(trial: optuna.trial.Trial, hyperparameters: dict):\n",
    "    model = LightningAutoencoder(**hyperparameters)\n",
    "    dm = FashionMNISTDataModule(**hyperparameters)\n",
    "    trainer = L.Trainer(\n",
    "        enable_checkpointing=False,\n",
    "        max_epochs=10,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=\"auto\",\n",
    "        logger=WandbLogger(\n",
    "            project=PROJECT_NAME, \n",
    "            name=PROJECT_NAME,\n",
    "        ),\n",
    "        callbacks=[\n",
    "            L.pytorch.callbacks.RichModelSummary(max_depth=5),\n",
    "            LearningRateMonitor(logging_interval=\"step\"), \n",
    "            TQDMProgressBar(refresh_rate=1,),\n",
    "            PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n",
    "        ],\n",
    "        precision=\"bf16\",\n",
    "        log_every_n_steps=5, \n",
    "    )\n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    trainer.fit(model, datamodule=dm)    \n",
    "    return trainer.callback_metrics[\"val_loss\"].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9d8b-0221-4fcc-a830-43b48588c125",
   "metadata": {},
   "source": [
    "# Hyper-parameter Search (Managed by Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a78b96c1-d166-4e14-9707-8de6b8e373f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g5/kw4s2jp95gq2m64qz52w7cd00000gn/T/ipykernel_89869/2660520381.py:18: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
      "  wandbc = WeightsAndBiasesCallback(\n",
      "\u001b[32m[I 2023-05-24 17:28:25,011]\u001b[0m Using an existing study with name 'soof-autoencoder-v6' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230524_172825-5g7s4y8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlab-tlv/soof-autoencoder-v6/runs/5g7s4y8x' target=\"_blank\">soof-autoencoder-v6</a></strong> to <a href='https://wandb.ai/mlab-tlv/soof-autoencoder-v6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlab-tlv/soof-autoencoder-v6' target=\"_blank\">https://wandb.ai/mlab-tlv/soof-autoencoder-v6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlab-tlv/soof-autoencoder-v6/runs/5g7s4y8x' target=\"_blank\">https://wandb.ai/mlab-tlv/soof-autoencoder-v6/runs/5g7s4y8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soof/dev/vae-playground/venv/lib/python3.10/site-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/soof/dev/vae-playground/venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:221: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name                </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type           </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">       In sizes </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">      Out sizes </span>â”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>â”‚ model               â”‚ MLPAutoencoder â”‚  223 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>â”‚ model.encoder       â”‚ MLPEncoder     â”‚  111 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>â”‚ model.encoder.seq   â”‚ Sequential     â”‚  111 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>â”‚ model.encoder.seq.0 â”‚ Flatten        â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>â”‚ model.encoder.seq.1 â”‚ Linear         â”‚  100 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>â”‚ model.encoder.seq.2 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>â”‚ model.encoder.seq.3 â”‚ Linear         â”‚  8.3 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>â”‚ model.encoder.seq.4 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>â”‚ model.encoder.seq.5 â”‚ Linear         â”‚  2.1 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>â”‚ model.encoder.seq.6 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>â”‚ model.encoder.seq.7 â”‚ Linear         â”‚    528 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>â”‚ model.decoder       â”‚ MLPDecoder     â”‚  112 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"> [1, 1, 28, 28] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 12 </span>â”‚ model.decoder.seq   â”‚ Sequential     â”‚  112 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 13 </span>â”‚ model.decoder.seq.0 â”‚ Linear         â”‚    544 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 16] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 14 </span>â”‚ model.decoder.seq.1 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 15 </span>â”‚ model.decoder.seq.2 â”‚ Linear         â”‚  2.1 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 32] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 16 </span>â”‚ model.decoder.seq.3 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 17 </span>â”‚ model.decoder.seq.4 â”‚ Linear         â”‚  8.3 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">     [1, 1, 64] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 18 </span>â”‚ model.decoder.seq.5 â”‚ GELU           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 19 </span>â”‚ model.decoder.seq.6 â”‚ Linear         â”‚  101 K â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 128] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚\n",
       "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 </span>â”‚ model.decoder.seq.7 â”‚ Tanh           â”‚      0 â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    [1, 1, 784] </span>â”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName               \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType          \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m      In sizes\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m     Out sizes\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0mâ”‚ model               â”‚ MLPAutoencoder â”‚  223 K â”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder       â”‚ MLPEncoder     â”‚  111 K â”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq   â”‚ Sequential     â”‚  111 K â”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.0 â”‚ Flatten        â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.1 â”‚ Linear         â”‚  100 K â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.2 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.3 â”‚ Linear         â”‚  8.3 K â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.4 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.5 â”‚ Linear         â”‚  2.1 K â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.6 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0mâ”‚ model.encoder.seq.7 â”‚ Linear         â”‚    528 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder       â”‚ MLPDecoder     â”‚  112 K â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m[1, 1, 28, 28]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m12\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq   â”‚ Sequential     â”‚  112 K â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m13\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.0 â”‚ Linear         â”‚    544 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 16]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m14\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.1 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m15\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.2 â”‚ Linear         â”‚  2.1 K â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 32]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m16\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.3 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m17\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.4 â”‚ Linear         â”‚  8.3 K â”‚\u001b[37m \u001b[0m\u001b[37m    [1, 1, 64]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m18\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.5 â”‚ GELU           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m19\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.6 â”‚ Linear         â”‚  101 K â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 128]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â”‚\u001b[2m \u001b[0m\u001b[2m20\u001b[0m\u001b[2m \u001b[0mâ”‚ model.decoder.seq.7 â”‚ Tanh           â”‚      0 â”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\u001b[37m \u001b[0m\u001b[37m   [1, 1, 784]\u001b[0m\u001b[37m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 223 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 223 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 223 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 223 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soof/dev/vae-playground/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/soof/dev/vae-playground/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a176c987c24e1794a8b4f90c30c6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[32m[I 2023-05-24 17:28:58,236]\u001b[0m Trial 2 finished with value: 1.6001386642456055 and parameters: {'momentum': 0.9439509149262681, 'learning_rate': 0.00017239943063262095, 'weight_decay': 0.00017329236583883226}. Best is trial 0 with value: 1.5612246990203857.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43078d21536c4043b9fdbe026fabb3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.217496â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>lr-SGD</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train_loss</td><td>â–‚â–ˆâ–†â–†â–ˆâ–‚â–„â–†â–„â–†â–‚â–â–„â–„â–ƒâ–…â–„â–†â–‚â–…â–„â–…â–…â–‚â–„â–â–ƒâ–…â–„â–‚â–‚â–†â–â–…â–ƒâ–†â–…â–…â–„â–„</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>val_loss</td><td>â–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>lr-SGD</td><td>0.00017</td></tr><tr><td>train_loss</td><td>1.5766</td></tr><tr><td>trainer/global_step</td><td>289</td></tr><tr><td>val_loss</td><td>1.60014</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soof-autoencoder-v6</strong> at: <a href='https://wandb.ai/mlab-tlv/soof-autoencoder-v6/runs/5g7s4y8x' target=\"_blank\">https://wandb.ai/mlab-tlv/soof-autoencoder-v6/runs/5g7s4y8x</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230524_172825-5g7s4y8x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 3\n",
      "Best trial:\n",
      "  Value: 1.5612246990203857\n",
      "  Params: \n",
      "    learning_rate: 0.003503804753603415\n",
      "    momentum: 0.5615432214197307\n",
      "    weight_decay: 0.00012992485836940082\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    # Generate hyperparameters\n",
    "    batch_size = 2048\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0) # 0.9 Worked well\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True) # 1e-2 Worked well\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True) # 5e-4 Worked well\n",
    "    \n",
    "    hyperparameters = dict(momentum=momentum, learning_rate=learning_rate, weight_decay=weight_decay, batch_size=batch_size)\n",
    "    \n",
    "    return train(trial, hyperparameters)\n",
    "\n",
    "def run():\n",
    "    wandb_kwargs = dict(\n",
    "        project=PROJECT_NAME,\n",
    "        name=PROJECT_NAME,\n",
    "        save_code=True,\n",
    "    )\n",
    "    wandbc = WeightsAndBiasesCallback(\n",
    "        metric_name=\"val_loss\",\n",
    "        wandb_kwargs=wandb_kwargs,\n",
    "        as_multirun=True\n",
    "    )\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        storage=OPTUNA_DATABASE_URL,            # Store run data here. Can be also postgres / mysql\n",
    "        load_if_exists=True,                    # Allow Resuming\n",
    "        study_name=PROJECT_NAME,\n",
    "        direction=\"minimize\", \n",
    "        pruner=optuna.pruners.MedianPruner()    # Prune unpromising runs (early stopping)\n",
    "    )\n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=1,\n",
    "        callbacks=[wandbc],\n",
    "    )\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "    \n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af126868-1080-4e8c-bf39-757f32706059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986db52-407b-4385-ad86-66cb453291bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
